{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(s):\n",
    "    s = s.lower()\n",
    "    data = re.sub(r'[^\\x00-\\x7F]+', ' ', s)\n",
    "    final_str = data.translate(None, string.punctuation)\n",
    "    filter_str = final_str.translate(None, digits)\n",
    "    nltk_tokens = nltk.word_tokenize(filter_str)\n",
    "    #Next find the roots of the word\n",
    "    str_= ''\n",
    "    for w in nltk_tokens:\n",
    "\n",
    "        if w not in stopwords.words('english'):\n",
    "            str_ += ' '  + (lemmatizer.lemmatize(w))\n",
    "    \n",
    "    return str_.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel('/home/prashant/Downloads/kagsa/data/glove.840B.300d.txt')\n",
    "#vocab = model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/prashant/Downloads/c3cc8568-0-dataset/train.csv')\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Complaint-reason']  = train['Complaint-reason'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common word removal\n",
    "freq = pd.Series(' '.join(train['Consumer-complaint-summary']).split()).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].\\\n",
    "apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = pd.Series(' '.join(train['Consumer-complaint-summary']).split()).value_counts()[-20000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = list(freq.index)\n",
    "# train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].map(lambda x : re.sub(\"x+\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].map(lambda x : x.translate(None, digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('final_prepro.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf_w_data = list(train['Consumer-complaint-summary'])\n",
    "# tf_idf = TfidfVectorizer(max_features=5000)\n",
    "# tf_idf_data = tf_idf.fit_transform(tf_w_data)\n",
    "import pandas as pd\n",
    "train = pd.read_csv('final_prepro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitted = []\n",
    "# for row in tf_w_data: \n",
    "#     splitted.append([word for word in row.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(model,all_data):\n",
    "    vector_sen = []\n",
    "    for d in all_data:\n",
    "        single_sen_vec = []\n",
    "        words = d.split(' ')\n",
    "        vec_created =  0.0\n",
    "        for w in words:\n",
    "            try:\n",
    "                get_word_vec = model[w]\n",
    "                vec_created += 1.0\n",
    "            except:\n",
    "                pass\n",
    "        if vec_created / len(words) > .5: \n",
    "            single_sen_vec.append(get_word_vec)\n",
    "            v = np.array(single_sen_vec).mean(axis=0)\n",
    "            vector_sen.append(v)\n",
    "            sent =1\n",
    "        else:\n",
    "            vector_sen.append(np.zeros(300))\n",
    "            sent = 0 \n",
    "    return (vector_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_vec = get_vector(model,list(train['Consumer-complaint-summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_w_data = []\n",
    "# tf_idf_data = tf_idf_data.toarray()\n",
    "# i = 0\n",
    "# for row in splitted:\n",
    "#     vec = [0 for i in range(50)]\n",
    "    \n",
    "#     temp_tfidf = []\n",
    "#     for val in tf_idf_data[i]:\n",
    "#         if val != 0:\n",
    "#             temp_tfidf.append(val)\n",
    "    \n",
    "#     count = 0\n",
    "#     tf_idf_sum = 0\n",
    "#     for word in row:\n",
    "#         try:\n",
    "#             count += 1\n",
    "#             tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "#             vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "#         except:\n",
    "#             pass\n",
    "#     vec = (float)(1/tf_idf_sum) * vec\n",
    "#     tf_w_data.append(vec)\n",
    "#     i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['merge'] = train[\"Consumer-complaint-summary\"]+ \" \"+\\\n",
    "train[\"Complaint-reason\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = list(train['merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,use_idf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "tfidf.fit(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,use_idf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "tfidf.fit(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_cr = pd.DataFrame(tfidf.transform(all_text).toarray())\n",
    "vec_cs = pd.DataFrame(embed_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.concat([vec_cr,vec_cs],axis = 1)\n",
    "print comp_df.shape\n",
    "comp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = comp_df\n",
    "labels = train['Complaint-Status']\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    (LinearSVC())\n",
    "\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='f1_weighted', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        print (model_name, fold_idx, accuracy)\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
