{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(s):\n",
    "    s = s.lower()\n",
    "    data = re.sub(r'[^\\x00-\\x7F]+', ' ', s)\n",
    "    final_str = data.translate(None, string.punctuation)\n",
    "    filter_str = final_str.translate(None, digits)\n",
    "    nltk_tokens = nltk.word_tokenize(filter_str)\n",
    "    #Next find the roots of the word\n",
    "    str_= ''\n",
    "    for w in nltk_tokens:\n",
    "\n",
    "        if w not in stopwords.words('english'):\n",
    "            str_ += ' '  + (lemmatizer.lemmatize(w))\n",
    "    \n",
    "    return str_.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/home/prashant/Downloads/c3cc8568-0-dataset/train.csv')\n",
    "df_train['tag'] = 'train'\n",
    "print df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/prashant/Downloads/c3cc8568-0-dataset/test.csv') \n",
    "df_test['tag'] = 'test'\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train , df_test],axis= 0)\n",
    "print df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.isnull().sum()\n",
    "df['Complaint-reason'] = df['Complaint-reason'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.isnull().sum()\n",
    "df['Consumer-complaint-summary'] = df['Consumer-complaint-summary'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df['Consumer-complaint-summary'] = df['Consumer-complaint-summary'].map(lambda x : re.sub(\"x+\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"merge\"] = df[\"Consumer-complaint-summary\"].map(str)+ \" \" + df[\"Complaint-reason\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/prashant/preprocess_brain_test_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df['tag'] == 'test']['Complaint-reason'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(model,all_data):\n",
    "    vector_sen = [\n",
    "    for d in all_data:\n",
    "        single_sen_vec = []\n",
    "        words = d.split(' ')\n",
    "        for w in words:\n",
    "            try:\n",
    "                \n",
    "                get_word_vec = model[w]\n",
    "            except:\n",
    "                get_word_vec =np.zeros(300)\n",
    "                pass\n",
    "            single_sen_vec.append(get_word_vec)\n",
    "        v = np.array(single_sen_vec).mean(axis=0)\n",
    "        vector_sen.append(v)\n",
    "    return vector_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel('/home/prashant/Downloads/kagsa/data/glove.840B.300d.txt')\n",
    "#vocab = model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=100, max_features=None, min_df=15,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = list(df['merge'])\n",
    "vectorizer = TfidfVectorizer(min_df=15,max_df=100)\n",
    "# # tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "#vector = get_vector(model,list(df['merge']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode document\n",
    "# vector = vectorizer.transform(text)\n",
    "# # summarize encoded vector\n",
    "# print(vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df['tag'] = list(df['tag'])\n",
    "vec_df['Complaint-Status'] = list(df['Complaint-Status'])\n",
    "vec_df['Transaction-Type'] = list(df['Transaction-Type'])\n",
    "vec_df['Complaint-ID'] = list(df['Complaint-ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_one_hot = vec_df\n",
    "\n",
    "train_data = complete_df_one_hot.loc[complete_df_one_hot['tag']== 'train']\n",
    "train_data.drop(['tag'],inplace=True,axis=1)\n",
    "test_data = complete_df_one_hot.loc[complete_df_one_hot['tag']== 'test']\n",
    "test_data.drop(['tag'],inplace=True,axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = (train_data['Complaint-Status'])\n",
    "x_train = train_data.drop(['Complaint-Status','Complaint-ID'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print y_train.shape\n",
    "y_train_one_hot = pd.get_dummies(y_train,prefix=['Complaint-Status'])\n",
    "print y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('/home/prashant/preprocess_brain.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "x_train = pd.get_dummies(x_train,prefix=['Transaction-Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.get_dummies(test_data,prefix=['Transaction-Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train_internal, y_test_internal = train_test_split(x_train,y_train ,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rm = rf.fit(X_train, y_train_internal)\n",
    "pred_internal = rm.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.f1_score(pred_internal,y_test_internal,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = list(test_data['Complaint-ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train_data.shape\n",
    "test_data.shape\n",
    "test_data.drop(['Complaint-Status','Complaint-ID'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rm.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(id_,columns=['Complaint-ID'])\n",
    "result['Complaint-Status'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/home/prashant/Downloads/c3cc8568-0-dataset/baseline_approx_rf_2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
